--- nova/api/openstack/compute/servers.py 2014-04-03 20:49:46.000000000 +0200
+++ nova/api/openstack/compute/servers.py 2014-06-08 23:44:36.000000000 +0200
@@ -39,7 +39,10 @@
 from nova.openstack.common import uuidutils
 from nova import policy
 from nova import utils
-
+
+import nova.image.glance
+from nova import cern
+
 
 server_opts = [
     cfg.BoolOpt('enable_instance_password',
@@ -758,7 +761,79 @@
         name = name.strip()
 
         image_uuid = self._image_from_req_data(body)
+
+        if len(name) > 64:
+            msg = "Instance name too long"
+            raise exc.HTTPBadRequest(explanation=msg)
+
+        if name.lower() != (utils.sanitize_hostname(name)).lower():
+            msg = "Instance name is not a valid hostname"
+            raise exc.HTTPBadRequest(explanation=msg)
+
+        if image_uuid != '':
+            image_service = nova.image.glance.get_default_image_service()
+            image_metadata = image_service.show(context, image_uuid)
+        else:
+            image_metadata = {}
+
+        if 'properties' in image_metadata.keys()\
+                         and 'os' in image_metadata['properties'].keys():
+            if (image_metadata['properties']['os']).lower() == 'windows' and\
+                                                            len(name) > 15:
+                msg = ("Instance name too long. Windows images only support "
+                  "15 character hostname")
+                raise exc.HTTPBadRequest(explanation=msg)
+
+        client_landb = cern.LanDB()
+        client_xldap = cern.Xldap()
+
+        if not client_landb.device_exists(name):
+            pass
+        else:
+            msg = "Hostname already in use"
+            raise exc.HTTPBadRequest(explanation=msg)
+
+        try:
+            socket.gethostbyname(name)
+        except:
+            pass
+        else:
+            msg = "Hostname already in DNS"
+            raise exc.HTTPBadRequest(explanation=msg)
+
+        metadata = server_dict.get('metadata', {})
 
+        if 'landb-responsible' in metadata.keys() and\
+            metadata['landb-responsible'] != '':
+            user_id = client_xldap.user_exists(metadata['landb-responsible'])
+            egroup_id = client_xldap.egroup_exists(metadata['landb-responsible'])
+            if user_id or egroup_id:
+                pass
+            else:
+                msg = "Cannot find user/egroup for responsible user"
+                raise exc.HTTPBadRequest(explanation=msg)
+
+        if 'landb-mainuser' in metadata.keys() and\
+            metadata['landb-mainuser'] != '':
+            user_id = client_xldap.user_exists(metadata['landb-mainuser'])
+            egroup_id = client_xldap.egroup_exists(metadata['landb-mainuser'])
+            if user_id or egroup_id:
+                pass
+            else:
+                msg = "Cannot find user/egroup for responsible user"
+                raise exc.HTTPBadRequest(explanation=msg)
+
+        if 'landb-alias' in metadata.keys():
+            new_alias = [x.strip() for x in metadata['landb-alias'].split(',')]
+            for alias in new_alias:
+                try:
+                    if client_landb.device_exists(alias):
+                        raise
+                except:
+                    msg = _("Alias - %s - The device already exists or is not "
+                            "a valid hostname" % str(alias))
+                    raise exc.HTTPBadRequest(explanation=msg)
+
         personality = server_dict.get('personality')
         config_drive = None
         if self.ext_mgr.is_loaded('os-config-drive'):
@@ -882,7 +957,23 @@
         if min_count > max_count:
             msg = _('min_count must be <= max_count')
             raise exc.HTTPBadRequest(explanation=msg)
+
+        if max_count > min_count and (len(name) + 37) > 64:
+            msg = ("Instance name too long for multiple instances creation. "
+                   "Only 27 characters allowed.")
+            raise exc.HTTPBadRequest(explanation=msg)
+
+        if max_count > min_count and 'properties' in image_metadata.keys()\
+                         and 'os' in image_metadata['properties'].keys():
+            if (image_metadata['properties']['os']).lower() == 'windows':
+                msg = ("Windows images don't support multiple instances "
+                       "creation")
+                raise exc.HTTPBadRequest(explanation=msg)
 
+        if max_count > min_count and 'landb-alias' in metadata.keys():
+            msg = ("With multiple instances is not possible to set alias.")
+            raise exc.HTTPBadRequest(explanation=msg)
+
         auto_disk_config = False
         if self.ext_mgr.is_loaded('OS-DCF'):
             auto_disk_config = server_dict.get('auto_disk_config')
@@ -996,10 +1087,10 @@
         update_dict = {}
 
         if 'name' in body['server']:
-            name = body['server']['name']
-            self._validate_server_name(name)
-            update_dict['display_name'] = name.strip()
-
+
+            msg = _("Hostname cannot be updated.")
+            raise exc.HTTPBadRequest(explanation=msg)
+
         if 'accessIPv4' in body['server']:
             access_ipv4 = body['server']['accessIPv4']
             if access_ipv4:
--- nova/cells/filters/project_target_cell.py 1970-01-01 01:00:00.000000000 +0100
+++ nova/cells/filters/project_target_cell.py 2014-06-09 10:42:05.000000000 +0200
@@ -0,0 +1,89 @@
+# Copyright (c) 2014 CERN
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
+"""
+Project target cell filter.
+
+Route a build form a project to a particular cell(s)
+"""
+
+from distutils import versionpredicate
+from oslo.config import cfg
+
+from nova.cells import filters
+from nova import exception
+from nova.openstack.common.gettextutils import _
+from nova.openstack.common import log as logging
+
+cell_project_target_cell_opts = [
+        cfg.ListOpt('default_cells',
+               default=[],
+               help='list of default cells'),
+        cfg.ListOpt('project_to_cell',
+               default=[],
+               help='list of cells and projects')
+
+]
+
+LOG = logging.getLogger(__name__)
+
+CONF = cfg.CONF
+CONF.register_opts(cell_project_target_cell_opts, group='cells')
+
+
+class ProjectTargetCell(filters.BaseCellFilter):
+    """Project target cell filter"""
+
+    def filter_all(self, cells, filter_properties):
+        """Override filter_all() which operates on the full list
+        of cells
+        """
+        request_spec = filter_properties.get('request_spec', {})
+        instance_properties = request_spec['instance_properties']
+        instance_project_id = instance_properties['project_id']
+
+        cells = list(cells)
+        cells_projects = {}
+        selected_cells = set()
+
+        scheduler = filter_properties['scheduler']
+        if len(cells) == 1 and\
+           cells[0].name == scheduler.state_manager.get_my_state().name:
+            return cells
+
+        try:
+            for x in CONF.cells.project_to_cell:
+                cell, project = x.strip().split('_')
+                if project not in cells_projects:
+                    cells_projects[project.lower()] = set()
+                cells_projects[project.lower()].add(cell.lower())
+
+            if instance_project_id in cells_projects.keys():
+                selected_cells = cells_projects[instance_project_id]
+            else:
+                for x in CONF.cells.default_cells:
+                    selected_cells.add(x.strip())
+        except Exception as e:
+            LOG.error(_("Cells setup wrong"))
+            raise exception.CernProjectTargetCell()
+
+        filtered_cells = [x for x in cells if x.name in selected_cells]
+
+        return filtered_cells
+
+    def _matches_version(self, version, version_requires):
+        predicate = versionpredicate.VersionPredicate(
+                             'prop (%s)' % version_requires)
+        return predicate.satisfied_by(version)
--- nova/cells/messaging.py   2014-04-03 20:49:46.000000000 +0200
+++ nova/cells/messaging.py   2014-06-09 10:48:27.000000000 +0200
@@ -49,7 +49,9 @@
 from nova.openstack.common import timeutils
 from nova.openstack.common import uuidutils
 from nova import utils
-
+
+from nova.network import model as network_model
+
 
 cell_messaging_opts = [
     cfg.IntOpt('max_hop_count',
@@ -874,10 +876,11 @@
     def resize_instance(self, message, instance, flavor,
                         extra_instance_updates):
         """Resize an instance via compute_api.resize()."""
+
         self._call_compute_api_with_obj(message.ctxt, instance, 'resize',
-                                        flavor_id=flavor['id'],
+                                        flavor_id=flavor['flavorid'],
                                         **extra_instance_updates)
-
+
     def live_migrate_instance(self, message, instance, block_migration,
                               disk_over_commit, host_name):
         """Live migrate an instance via compute_api.live_migrate()."""
@@ -994,6 +997,14 @@
                 self.db.instance_create(message.ctxt, instance,
                                         legacy=False)
         if info_cache:
+
+            network_info = info_cache.get('network_info')
+            if isinstance(network_info, list):
+                if not isinstance(network_info, network_model.NetworkInfo):
+                    network_info = network_model.NetworkInfo.hydrate(
+                            network_info)
+                info_cache['network_info'] = network_info.json()
+
             try:
                 self.db.instance_info_cache_update(
                         message.ctxt, instance_uuid, info_cache)
--- nova/cells/scheduler.py   2014-04-03 20:49:46.000000000 +0200
+++ nova/cells/scheduler.py   2014-06-09 10:56:46.000000000 +0200
@@ -84,27 +84,29 @@
 
     def _create_instances_here(self, ctxt, instance_uuids, instance_properties,
             instance_type, image, security_groups, block_device_mapping):
-        instance_values = copy.copy(instance_properties)
-        # The parent may pass these metadata values as lists, and the
-        # create call expects it to be a dict.
-        instance_values['metadata'] = utils.instance_meta(instance_values)
-        sys_metadata = utils.instance_sys_meta(instance_values)
-        # Make sure the flavor info is set.  It may not have been passed
-        # down.
-        sys_metadata = flavors.save_flavor_info(sys_metadata, instance_type)
-        instance_values['system_metadata'] = sys_metadata
-        # Pop out things that will get set properly when re-creating the
-        # instance record.
-        instance_values.pop('id')
-        instance_values.pop('name')
-        instance_values.pop('info_cache')
-        instance_values.pop('security_groups')
-
         num_instances = len(instance_uuids)
         for i, instance_uuid in enumerate(instance_uuids):
+
+            instance_values = copy.copy(instance_properties[i])
+            # The parent may pass these metadata values as lists, and the
+            # create call expects it to be a dict.
+            instance_values['metadata'] = utils.instance_meta(instance_values)
+            sys_metadata = utils.instance_sys_meta(instance_values)
+            # Make sure the flavor info is set.  It may not have been passed
+            # down.
+            sys_metadata = flavors.save_flavor_info(sys_metadata, instance_type)
+            instance_values['system_metadata'] = sys_metadata
+            # Pop out things that will get set properly when re-creating the
+            # instance record.
+            instance_values.pop('id')
+            instance_values.pop('name')
+            instance_values.pop('info_cache')
+            instance_values.pop('security_groups')
+
             instance = instance_obj.Instance()
             instance.update(instance_values)
             instance.uuid = instance_uuid
+
             instance = self.compute_api.create_db_entry_for_new_instance(
                     ctxt,
                     instance_type,
@@ -112,8 +114,8 @@
                     instance,
                     security_groups,
                     block_device_mapping,
-                    num_instances, i)
-
+                    1, i)
+
             instance = obj_base.obj_to_primitive(instance)
             self.msg_runner.instance_update_at_top(ctxt, instance)
 
@@ -197,7 +199,9 @@
             build_inst_kwargs):
         """Attempt to build instance(s) or send msg to child cell."""
         ctxt = message.ctxt
-        instance_properties = build_inst_kwargs['instances'][0]
+
+        instance_properties = build_inst_kwargs['instances']
+
         filter_properties = build_inst_kwargs['filter_properties']
         instance_type = filter_properties['instance_type']
         image = build_inst_kwargs['image']
--- nova/cern.py  1970-01-01 01:00:00.000000000 +0100
+++ nova/cern.py  2014-06-09 18:17:29.000000000 +0200
@@ -0,0 +1,334 @@
+# Copyright (c) 2014 CERN
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
+import logging as pylog
+from nova.openstack.common.gettextutils import _
+from nova.openstack.common import log as logging
+from nova import exception
+
+import ldap
+import time
+import random
+import string
+import suds.client
+from suds.xsd.doctor import ImportDoctor, Import
+from suds.client import Client
+
+LOG = logging.getLogger(__name__)
+pylog.getLogger('suds.client').setLevel(pylog.CRITICAL)
+
+
+class LanDB:
+    def __init__(self, username=None, password=None, client=None):
+        if client != None:
+            self.client = client
+        else:
+            self.client = self.__auth(username=None, password=None)
+
+
+    def __auth(self, username=None, password=None):
+        """Authenticates in landb"""
+        url = 'https://network.cern.ch/sc/soap/soap.fcgi?v=4&WSDL'
+        imp = Import('http://schemas.xmlsoap.org/soap/encoding/')
+        d = ImportDoctor(imp)
+        client = Client(url, doctor=d)
+
+        if (username == None or password == None):
+            try:
+                file = open('/etc/sysconfig/nova_network', 'r')
+                (username, password) = (file.readline()).split(' ')
+                password = password.rstrip("\n")
+                file.close()
+            except Exception as e:
+                LOG.error(_("Cannot find landb credentials: %s" % str(e)))
+                raise exception.CernNetwork()
+
+        try:
+            token = client.service.getAuthToken(username,password,'CERN')
+            myheader = dict(Auth={'token':token})
+            client.set_options(soapheaders=myheader)
+        except Exception as e:
+            LOG.error(_("Cannot authenticate in landb: %s" % str(e)))
+            raise exception.CernLanDB()
+
+        return client
+
+
+    def vm_update(self, device, new_device=None,
+            location=None, manufacter=None, model=None, description=None, tag=None,
+            operating_system=None, responsible_person=None, user_person=None):
+        """Update vm metadata in landb"""
+        metadata = None
+        try:
+            metadata = self.client.service.getDeviceBasicInfo(device.upper())
+        except Exception as e:
+            pass
+
+        if new_device == None: new_device = device
+
+        if location == None:
+            location = {'Floor':'0', 'Room':'0', 'Building':'0'}
+
+        if manufacter == None:
+            manufacter = 'KVM'
+
+        if model == None:
+            model = 'VIRTUAL MACHINE'
+
+        if description == None and metadata != None:
+            description = metadata.Description if metadata.Description != None\
+             else ''
+
+        if tag == None:
+            tag = 'OPENSTACK VM'
+
+        if operating_system == None and metadata != None:
+            operating_system = metadata.OperatingSystem
+
+        if responsible_person == None and metadata != None:
+            responsible_person = metadata.ResponsiblePerson
+
+        if user_person == None and metadata != None:
+            user_person = metadata.UserPerson
+
+        try:
+            self.client.service.vmUpdate(device,
+                        {'DeviceName': new_device,
+                        'Location': location,
+                        'Manufacturer': manufacter,
+                        'Model': model,
+                        'Description': description,
+                        'Tag': tag,
+                        'OperatingSystem': operating_system,
+                        'ResponsiblePerson':responsible_person,
+                        'UserPerson':user_person})
+        except Exception as e:
+            LOG.error(_("Cannot update landb: %s" % str(e)))
+            raise exception.CernLanDB()
+
+
+    def vm_migrate(self, device, parent):
+        """Migrate vm to parent"""
+        try:
+            self.client.service.vmMigrate(device, (parent.lower()).replace('.cern.ch', ''))
+            LOG.debug(_("Parent migration to |%s|" % (parent.lower()).replace('.cern.ch', '')))
+        except Exception as e:
+            LOG.error(_("Cannot migrate VM in landb - %s" % str(e)))
+
+
+    def vm_delete(self, device, new_device=None):
+        """Update vm as deleted in landb"""
+        if new_device == None:
+            for i in range(5):
+                new_device = 'Z' + ''.join(random.choice(string.ascii_uppercase\
+                 + string.digits) for x in range(10))
+                LOG.debug(_("Random instance name for Landb: %s" % new_device))
+                if not self.device_exists:
+                    LOG.debug(_("Hostname already exists: %s" % new_device))
+                    continue
+                else:
+                    break
+
+        os = {'Name': 'UNKNOWN',
+              'Version': 'UNKNOWN'}
+
+        responsible = {'FirstName':'E-GROUP',
+                       'Name':'AI-OPENSTACK-ADMIN',
+                       'Department':'IT'}
+
+        user_person = {'FirstName':'E-GROUP',
+                       'Name':'AI-OPENSTACK-ADMIN',
+                       'Department':'IT'}
+
+        try:
+            self.client.service.vmNetReset(device)
+            self.vm_update(device, new_device=new_device,
+                description='Not in use', operating_system=os,
+                responsible_person=responsible, user_person=user_person)
+        except Exception as e:
+            LOG.error(_("Cannot delete vm from landb: %s" % str(e)))
+            raise exception.CernLanDB()
+
+
+    def device_exists(self, device):
+        """Check if a device is registered in landb"""
+        try:
+            self.client.service.getDeviceInfo(device)
+        except:
+            return False
+        return device
+
+
+    def device_hostname(self, address):
+        """Get the hostname given an IP"""
+        try:
+            device = (self.client.service.searchDevice({'IPAddress':address}))[0]
+        except Exception as e:
+            LOG.error(_("Cannot find device with IP: %s" % str(e)))
+            raise exception.CernInvalidHostname('')
+        return device
+
+
+    def device_migrate(self, hostname, node):
+        """Migrate VM to node"""
+        try:
+            self.client.service.vmMigrate(hostname, node)
+        except Exception as e:
+            LOG.error(_("Cannot migrate device in lanDB: %s" % str(e)))
+            return False
+        return True
+
+
+    def alias_update(self, device, new_alias):
+        """Update alias"""
+        try:
+            old_alias = self.getDeviceInfo(device).Interfaces[0].IPAliases
+            if old_alias == None: old_alias = []
+
+            for alias in new_alias:
+                if (alias not in old_alias) and self.device_exists(alias):
+                    LOG.error(_("Alias already exists"))
+                    raise exception.CernInvalidHostname()
+
+            for alias in old_alias:
+                self.__unset_alias(device, alias)
+
+            for alias in new_alias:
+                self.__set_alias(device, alias)
+        except exception.CernInvalidHostname:
+             msg = _("%s - The device already exists or is not "
+                     "a valid hostname" % str(alias))
+             raise exception.CernInvalidHostname(msg)
+
+
+    def __set_alias(self, device, alias):
+        """Set alias to a device"""
+        try:
+            self.client.service.interfaceAddAlias(device, alias)
+        except Exception as e:
+            LOG.error(_("Cannot set alias in landb: %s" % str(e)))
+            raise exception.CernLanDB()
+
+
+    def __unset_alias(self, device, alias):
+        """Unset all alias in a device"""
+        try:
+            self.client.service.interfaceRemoveAlias(device, alias)
+        except Exception as e:
+            LOG.error(_("Cannot unset alias in landb: %s" % str(e)))
+            raise exception.CernLanDB()
+
+
+    def vmClusterGetDevices(self, cluster):
+        """Get all cluster devices"""
+        try:
+            return self.client.service.vmClusterGetDevices(cluster)
+        except Exception as e:
+            LOG.error(_("Cannot get VMs from network cluster - %s - %s"),
+                        cluster, str(e))
+            raise exception.CernLanDB()
+
+
+    def vmClusterGetInfo(self, cluster):
+        """Get cluster info"""
+        try:
+            return self.client.service.vmClusterGetInfo(cluster)
+        except Exception as e:
+            LOG.error(_("Cannot get network services for network cluster - "
+                        "%s - %s"), cluster, str(e))
+            raise exception.CernLanDB()
+
+
+    def getServiceInfo(self, service):
+        """Get service information"""
+        try:
+            return self.client.service.getServiceInfo(service)
+        except Exception as e:
+            LOG.error(_("Cannot get service information"))
+            raise exception.CernLanDB()
+
+
+    def getDevicesFromService(self, service):
+        """Get devices from service"""
+        try:
+            return self.client.service.getDevicesFromService(service)
+        except Exception as e:
+            LOG.error(_("Cannot devices from service: %s - %s"), service, str(e))
+            raise exception.CernLanDB()
+
+
+    def getDeviceInfo(self, device):
+        """Get device information"""
+        try:
+            return self.client.service.getDeviceInfo(device)
+        except Exception as e:
+            LOG.error(_("Cannot get VM netwok info - %s" % str(e)))
+            raise exception.CernLanDB()
+
+
+class Xldap:
+    def __init__(self, url='xldap.cern.ch', protocol_version=ldap.VERSION3,
+                 searchScope=ldap.SCOPE_SUBTREE, retrieveAttributes=None):
+        self.client = ldap.open(url)
+        self.client.protocol_version = protocol_version
+        self.searchScope = searchScope
+        self.retrieveAttributes = retrieveAttributes
+
+    def user_exists(self, user, baseDN='OU=Users,OU=Organic Units,DC=cern,DC=ch'):
+        """Check if an user exists at CERN"""
+        try:
+            searchFilter = "cn="+user
+            ldap_result_id = self.client.search(baseDN, self.searchScope,
+                        searchFilter, self.retrieveAttributes)
+            result_type, result_data = self.client.result(ldap_result_id, 0)
+            if (result_data == []):
+                return False
+            if result_type == ldap.RES_SEARCH_ENTRY:
+                return int(result_data[0][1]['employeeID'][0])
+        except Exception as e:
+            LOG.error(_("Cannot verify if USER exists. %s" % str(e)))
+            raise exception.CernInvalidUser()
+
+    def egroup_exists(self, egroup, baseDN='OU=Workgroups,DC=cern,DC=ch'):
+        """Check if an egroup exists at CERN"""
+        try:
+            searchFilter = "cn="+egroup
+            ldap_result_id = self.client.search(baseDN, self.searchScope,
+                        searchFilter, self.retrieveAttributes)
+            result_type, result_data = self.client.result(ldap_result_id, 0)
+            if (result_data == []):
+                return False
+            if result_type == ldap.RES_SEARCH_ENTRY:
+                return str(egroup)
+        except Exception as e:
+            LOG.error(_("Cannot verify if EGROUP exists. %s" % str(e)))
+            raise exception.CernInvalidEgroup()
+
+    def device_exists(self):
+        pass
+
+class ActiveDirectory():
+    def __init__(self):
+        url='http://compmgtsvc.web.cern.ch/compmgtsvc/compmgtsvc.asmx?wsdl'
+        self.client = Client(url, cache=None)
+
+    def register(self, hostname):
+        try:
+            if self.client.service.CheckComputer(hostname) != None:
+                LOG.error(_("AD update failed - %s" % hostname))
+        except Exception as e:
+            LOG.error(_("Cannot check if VM is in AD. %s" % str(e)))
+            raise exception.CernActiveDirectory()
+
--- nova/cmd/manage.py    2014-04-03 20:49:46.000000000 +0200
+++ nova/cmd/manage.py    2014-06-09 11:00:16.000000000 +0200
@@ -1239,7 +1239,82 @@
                     transport['port'], transport['virtual_host']))
         print(fmt % ('-' * 3, '-' * 10, '-' * 6, '-' * 10, '-' * 15,
                 '-' * 5, '-' * 10))
-
+
+class CernCommands(object):
+    """Class for CERN newtork integration."""
+
+    @args('--label', dest="label", metavar='<label>',
+            help='Label for network (ex: public)')
+    @args('--bridge', dest="bridge",
+            metavar='<bridge>',
+            help='VIFs on this network are connected to this bridge')
+    @args('--bridge_interface', dest="bridge_interface",
+            metavar='<bridge interface>',
+            help='the bridge is connected to this interface')
+    def network_create(self, label=None, conf=None, num_networks=None,
+               network_size=None, multi_host=None, vlan_start=None,
+               vpn_start=None, fixed_range_v6=None, gateway=None,
+               gateway_v6=None, bridge=None, bridge_interface=None,
+               dns1=None, dns2=None, project_id=None, priority=None,
+               uuid=None):
+        """Creates cern network"""
+
+        net_manager = importutils.import_object(CONF.network_manager)
+        net_manager.create_networks(context.get_admin_context(),
+                                    label='CERN_NETWORK',
+                                    cidr='0.0.0.0/0',
+                                    multi_host=0,
+                                    num_networks=int('1'),
+                                    network_size=int('0'),
+                                    vlan_start=int('0'),
+                                    vpn_start=int('0'),
+                                    cidr_v6='',
+                                    gateway='0.0.0.0',
+                                    gateway_v6='',
+                                    bridge=bridge,
+                                    bridge_interface=bridge_interface,
+                                    dns1='',
+                                    dns2='',
+                                    project_id=project_id,
+                                    priority=0,
+                                    uuid=uuid)
+
+    def network_list(self):
+        """Updates cern network"""
+        ctxt = context.get_admin_context()
+
+        try:
+            fixed_ips = db.fixed_ip_get_all(ctxt)
+        except Exception as e:
+            print (_("Error: %s") % e)
+            sys.exit(2)
+
+        instances = db.instance_get_all(context.get_admin_context())
+        instances_by_id = {}
+        for instance in instances:
+            instances_by_id[instance['id']] = instance
+
+        print (_("%-20s\t%-20s\t%-20s\t%-20s\t%-20s") % (_('ip address'),
+                                                     _('mac address'),
+                                                     _('compute node'),
+                                                     _('network cluster'),
+                                                     _('allocated')))
+
+        for fixed_ip in fixed_ips:
+            print (_("%-20s\t%-20s\t%-20s\t%-20s\t%-20s") % \
+                                                     (fixed_ip['address'],
+                                                      fixed_ip['mac'],
+                                                      fixed_ip['host'],
+                                                      fixed_ip['netcluster'],
+                                                      fixed_ip['allocated']))
+
+    @args('--clustername', dest="clustername", metavar="<clustername>",
+      help='CERN network cluster name')
+    def network_update(self, clustername):
+        """Update cern network"""
+        net_manager = importutils.import_object(CONF.network_manager)
+        net_manager.update_network(context.get_admin_context(), clustername)
+
 
 CATEGORIES = {
     'account': AccountCommands,
@@ -1259,6 +1334,9 @@
     'shell': ShellCommands,
     'vm': VmCommands,
     'vpn': VpnCommands,
+
+    'cern': CernCommands,
+
 }
 
 
--- nova/compute/api.py   2014-04-03 20:49:46.000000000 +0200
+++ nova/compute/api.py   2014-06-09 11:12:28.000000000 +0200
@@ -72,7 +72,9 @@
 from nova import servicegroup
 from nova import utils
 from nova import volume
-
+
+from nova import cern
+
 LOG = logging.getLogger(__name__)
 
 get_notifier = functools.partial(notifier.get_notifier, service='compute')
@@ -84,6 +86,12 @@
                 default=False,
                 help='Allow destination machine to match source for resize. '
                      'Useful when testing in single-host environments.'),
+
+    cfg.ListOpt('default_schedule_zones',
+               deprecated_name='default_schedule_zone',
+               default=[],
+               help='availability zone to use when user doesn\'t specify one'),
+
     cfg.BoolOpt('allow_migrate_to_same_host',
                 default=False,
                 help='Allow migrate machine to the same host. '
@@ -498,10 +506,10 @@
             else:
                 raise exception.InvalidInput(
                         reason="Unable to parse availability_zone")
-
-        if not availability_zone:
-            availability_zone = CONF.default_schedule_zone
-
+
+#        if not availability_zone:
+#            availability_zone = CONF.default_schedule_zone
+
         if forced_host:
             check_policy(context, 'create:forced_host', {})
         if forced_node:
@@ -1079,7 +1087,9 @@
             instance.hostname = utils.sanitize_hostname(hostname)
 
     def _default_display_name(self, instance_uuid):
-        return "Server %s" % instance_uuid
+
+        return "server-%s" % instance_uuid
+
 
     def _populate_instance_for_create(self, instance, image,
                                       index, security_groups, instance_type):
@@ -2166,7 +2176,22 @@
                                            migration,
                                            migration.source_compute,
                                            reservations)
+
+        if CONF.network_manager != 'nova.network.manager.CernManager':
+            return
 
+        try:
+            fixed_ips = self.db.fixed_ip_get_by_instance(context,
+                                                         instance['uuid'])
+            vm_ip = (fixed_ips[0])['address']
+
+            client = cern.LanDB()
+            host = (instance['host'].lower()).replace('.cern.ch', '')
+            device_name = client.device_hostname(vm_ip)
+            client.device_migrate(device_name, host)
+        except Exception as e:
+            LOG.error(_("Cannot migrate VM in landb - %s" % str(e)))
+
     @staticmethod
     def _resize_quota_delta(context, new_instance_type,
                             old_instance_type, sense, compare):
@@ -2303,8 +2328,9 @@
         # when Ops is migrating off of a failed host.
         if not same_instance_type and new_instance_type.get('disabled'):
             raise exception.FlavorNotFound(flavor_id=flavor_id)
-
-        if same_instance_type and flavor_id:
+
+        if same_instance_type and flavor_id and self.cell_type != 'compute':
+
             raise exception.CannotResizeToSameFlavor()
 
         # ensure there is sufficient headroom for upsizes
@@ -2338,7 +2364,12 @@
         instance.save(expected_task_state=None)
 
         filter_properties = {'ignore_hosts': []}
-
+
+        ipservice = self.db.cern_netcluster_get(context, instance['host'])
+        ignore_hosts = self.db.cern_ignore_hosts(context,
+                                                 ipservice['netcluster'])
+        filter_properties['ignore_hosts'].extend(ignore_hosts)
+
         if not CONF.allow_resize_to_same_host:
             filter_properties['ignore_hosts'].append(instance['host'])
 
@@ -2825,6 +2856,50 @@
                           task_state=None)
     def delete_instance_metadata(self, context, instance, key):
         """Delete the given metadata item from an instance."""
+#  CERN
+        landb_update = False
+        landb_description = None
+        landb_responsible = None
+        landb_mainuser = None
+
+        if key == 'landb-alias':
+            new_alias = []
+            client = cern.LanDB()
+            client.alias_update(instance['hostname'], new_alias)
+
+        if key == 'landb-mainuser':
+            landb_update = True
+
+            client = cern.Xldap()
+            user_id = client.user_exists(instance['user_id'])
+
+            if user_id:
+                landb_mainuser = {'PersonID':user_id}
+            else:
+                LOG.error(_("Cannot find user/egroup for main user. %s" % str(e)))
+                raise exception.CernInvalidUserEgroup()
+
+        if key == 'landb-responsible':
+            landb_update = True
+
+            client = cern.Xldap()
+            user_id = client.user_exists(instance['user_id'])
+
+            if user_id:
+                landb_responsible = {'PersonID':user_id}
+            else:
+                LOG.error(_("Cannot find user/egroup for responsible user. %s" % str(e)))
+                raise exception.CernInvalidUserEgroup()
+
+        if key == 'landb-description':
+            landb_update = True
+            landb_description = ''
+
+        if landb_update == True:
+            client = cern.LanDB()
+            client.vm_update(instance['hostname'], description=landb_description,
+                responsible_person=landb_responsible, user_person=landb_mainuser)
+
         self.db.instance_metadata_delete(context, instance['uuid'], key)
         instance['metadata'] = {}
         notifications.send_update(context, instance, instance)
@@ -2853,6 +2928,57 @@
             _metadata.update(metadata)
 
         self._check_metadata_properties_quota(context, _metadata)
+
+        landb_update = False
+        landb_description = None
+        landb_responsible = None
+        landb_mainuser = None
+
+        if 'landb-alias' in metadata.keys():
+            new_alias = [x.strip() for x in metadata['landb-alias'].split(',')]
+            client = cern.LanDB()
+            client.alias_update(instance['hostname'], new_alias)
+
+        if 'landb-mainuser' in metadata.keys():
+            landb_update = True
+
+            client = cern.Xldap()
+            user_id = client.user_exists(metadata['landb-mainuser'])
+            egroup_id = client.egroup_exists(metadata['landb-mainuser'])
+
+            if user_id:
+                landb_mainuser = {'PersonID':user_id}
+            elif egroup_id:
+                landb_mainuser = {'FirstName':'E-GROUP', 'Name':egroup_id}
+            else:
+                LOG.error(_("Cannot find user/egroup for main user"))
+                raise exception.CernInvalidUserEgroup()
+
+        if 'landb-responsible' in metadata.keys():
+            landb_update = True
+
+            client = cern.Xldap()
+            user_id = client.user_exists(metadata['landb-responsible'])
+            egroup_id = client.egroup_exists(metadata['landb-responsible'])
+
+            if user_id:
+                landb_responsible = {'PersonID':user_id}
+            elif egroup_id:
+                landb_responsible = {'FirstName':'E-GROUP', 'Name':egroup_id}
+            else:
+                LOG.error(_("Cannot find user/egroup for responsible user"))
+                raise exception.CernInvalidUserEgroup()
+
+        if 'landb-description' in metadata.keys():
+            landb_update = True
+            landb_description = metadata['landb-description']
+
+        # get previous vm information
+        if landb_update == True:
+            client = cern.LanDB()
+            client.vm_update(instance['hostname'], description=landb_description,
+                responsible_person=landb_responsible, user_person=landb_mainuser)
+
         metadata = self.db.instance_metadata_update(context, instance['uuid'],
                                          _metadata, True)
         instance['metadata'] = metadata
--- nova/compute/manager.py   2014-04-03 20:49:46.000000000 +0200
+++ nova/compute/manager.py   2014-06-09 11:16:32.000000000 +0200
@@ -83,7 +83,10 @@
 from nova.virt import virtapi
 from nova import volume
 from nova.volume import encryptors
-
+
+import time
+from nova import cern
+
 
 compute_opts = [
     cfg.StrOpt('console_host',
@@ -174,6 +177,11 @@
                default=0,
                help="Automatically confirm resizes after N seconds. "
                     "Set to 0 to disable."),
+
+    cfg.IntOpt('landb_dns_timeout',
+               default=1200,
+               help="DNS timeout in seconds.")
+
 ]
 
 running_deleted_opts = [
@@ -212,7 +220,9 @@
 CONF.import_opt('vnc_enabled', 'nova.vnc')
 CONF.import_opt('enabled', 'nova.spice', group='spice')
 CONF.import_opt('enable', 'nova.cells.opts', group='cells')
-
+
+CONF.import_opt('cern_landb', 'nova.network.manager')
+
 LOG = logging.getLogger(__name__)
 
 get_notifier = functools.partial(notifier.get_notifier, service='compute')
@@ -920,7 +930,41 @@
                 raise exception.Base64Exception(path=path)
 
         return [_decode(f) for f in injected_files]
+
+    def _cern_ready(self, context, instance):
+            if instance['hostname'] == "server-"+str(instance['uuid']):
+                return
 
+            if not CONF.cern_landb:
+                return
+
+            instance_hostname = str(instance['hostname'])
+            meta = self.conductor_api.instance_metadata_get(context,
+                                                            instance['uuid'])
+            if ('cern-update-hostname' in meta.keys()\
+                        and meta['cern-update-hostname'].lower() == 'false'):
+                return
+
+            if ('cern-services' in meta.keys()\
+                and meta['cern-services'].lower() != 'false')\
+                or ('cern-services' not in meta.keys()):
+                time.sleep(5)
+                client = cern.ActiveDirectory()
+                client.register(instance_hostname)
+
+                wait_time = 0
+                while(wait_time < CONF.landb_dns_timeout):
+                    try:
+                        socket.gethostbyname(instance_hostname)
+                        break
+                    except:
+                        LOG.info(_("Waiting for DNS - %s" % instance['uuid']))
+                        time.sleep(15)
+                        wait_time=wait_time+15
+                else:
+                    LOG.error(_("DNS update failed - %s" % instance['uuid']))
+                    raise exception.CernDNS()
+
     def _run_instance(self, context, request_spec,
                       filter_properties, requested_networks, injected_files,
                       admin_password, is_first_time, node, instance,
@@ -1024,7 +1068,9 @@
                 network_info = self._allocate_network(context, instance,
                         requested_networks, macs, security_groups,
                         dhcp_options)
-
+
+                self._cern_ready(context, instance)
+
                 self._instance_update(
                         context, instance['uuid'],
                         vm_state=vm_states.BUILDING,
--- nova/conductor/api.py 2014-04-03 20:49:46.000000000 +0200
+++ nova/conductor/api.py 2014-06-09 11:18:18.000000000 +0200
@@ -72,7 +72,10 @@
                              columns_to_join=None):
         return self._manager.instance_get_by_uuid(context, instance_uuid,
                 columns_to_join)
-
+
+    def instance_metadata_get(self, context, instance_uuid):
+        return self._manager.instance_metadata_get(context, instance_uuid)
+
     def instance_destroy(self, context, instance):
         return self._manager.instance_destroy(context, instance)
 
--- nova/conductor/manager.py 2014-04-03 20:49:46.000000000 +0200
+++ nova/conductor/manager.py 2014-06-09 11:19:32.000000000 +0200
@@ -143,7 +143,12 @@
         return jsonutils.to_primitive(
             self.db.instance_get_by_uuid(context, instance_uuid,
                 columns_to_join))
-
+
+    @rpc_common.client_exceptions(exception.InstanceNotFound)
+    def instance_metadata_get(self, context, instance_uuid):
+        return jsonutils.to_primitive(
+            self.db.instance_metadata_get(context, instance_uuid))
+
     # NOTE(hanlind): This method can be removed in v2.0 of the RPC API.
     def instance_get_all(self, context):
         return jsonutils.to_primitive(self.db.instance_get_all(context))
--- nova/conductor/rpcapi.py  2014-04-03 20:49:46.000000000 +0200
+++ nova/conductor/rpcapi.py  2014-06-09 11:20:42.000000000 +0200
@@ -152,7 +152,12 @@
             kwargs = {'instance_uuid': instance_uuid}
         cctxt = self.client.prepare(version=version)
         return cctxt.call(context, 'instance_get_by_uuid', **kwargs)
-
+
+    def instance_metadata_get(self, context, instance_uuid):
+        cctxt = self.client.prepare(version='1.2')
+        return cctxt.call(context, 'instance_metadata_get',
+                          instance_uuid=instance_uuid)
+
     def migration_get_in_progress_by_host_and_node(self, context,
                                                    host, node):
         cctxt = self.client.prepare(version='1.31')
--- nova/db/api.py    2014-04-03 20:49:46.000000000 +0200
+++ nova/db/api.py    2014-06-09 11:22:06.000000000 +0200
@@ -1925,3 +1925,26 @@
     """
     return IMPL.archive_deleted_rows_for_table(context, tablename,
                                                max_rows=max_rows)
+
+def cern_fixed_host_bulk_create(context, hosts):
+    """Create a lot of fixed hosts from the values dictionary."""
+    return IMPL.cern_fixed_host_bulk_create(context, hosts)
+
+def cern_netcluster_get(context, host):
+    """Get the host ipservice"""
+    return IMPL.cern_netcluster_get(context, host)
+
+def cern_mac_ip_get(context, ipservice, host=None):
+    """Get the host ipservice"""
+    return IMPL.cern_mac_ip_get(context, ipservice, host)
+
+def cern_ignore_hosts(context, ipservice):
+    return IMPL.cern_ignore_hosts(context, ipservice)
+
+def cern_fixed_ip_get_by_address(context, xip):
+    return IMPL.cern_fixed_ip_get_by_address(context, xip)
+
+def aggregate_metadata_get_all_by_key(context, key=None):
+    """Get all metadata values in all aggregates given a key."""
+    return IMPL.aggregate_metadata_get_all_by_key(context, key)
+
--- nova/db/sqlalchemy/api.py 2014-04-03 20:49:46.000000000 +0200
+++ nova/db/sqlalchemy/api.py 2014-06-09 11:23:56.000000000 +0200
@@ -6004,3 +6004,80 @@
         device.update(values)
         session.add(device)
     return device
+
+@require_context
+def cern_fixed_host_bulk_create(_context, hosts):
+    session = get_session()
+    with session.begin():
+        for host in hosts:
+            model = models.CernNetwork()
+            model.update(host)
+            session.add(model)
+
+@require_context
+def cern_netcluster_get(context, xhost):
+    session = get_session()
+    return session.query(models.CernNetwork).\
+                   filter_by(host=xhost).\
+                   first()
+
+@require_context
+def cern_mac_ip_get(context, xipservice, host):
+    session = get_session()
+    with session.begin():
+        fixed_ip_ref = session.query(models.FixedIp).\
+                               filter_by(reserved=False).\
+                               filter_by(deleted=False).\
+                               filter_by(allocated=False).\
+                               filter_by(netcluster=xipservice).\
+                               with_lockmode('update').\
+                               first()
+
+        if not fixed_ip_ref:
+            raise exception.NoMoreFixedIps()
+
+        if host:
+            fixed_ip_ref.host = host
+        session.add(fixed_ip_ref)
+    return fixed_ip_ref
+
+@require_context
+def cern_ignore_hosts(context, xipservice):
+    session = get_session()
+    rows = session.query(models.CernNetwork).\
+           filter(models.CernNetwork.netcluster != xipservice).\
+           all()
+
+    hosts = []
+    for r in rows:
+        hosts.append(r['host'].lower())
+
+    return hosts
+
+@require_context
+def cern_fixed_ip_get_by_address(context, xip):
+    session = get_session()
+    ips = session.query(models.FixedIp).\
+           filter(models.FixedIp.address == xip).\
+           first()
+
+    if not ips:
+        return None
+
+    return ips
+
+@require_admin_context
+def aggregate_metadata_get_all_by_key(context, key=None):
+    query = model_query(context, models.Aggregate).join(
+            "_metadata")
+    if key:
+        query = query.filter(models.AggregateMetadata.key == key)
+    rows = query.all()
+
+    metadata = collections.defaultdict(set)
+    for aggr in rows:
+        for kv in aggr._metadata:
+            if not key or kv['key'] == key:
+                metadata[kv['key']].add(kv['value'])
+    return metadata
+
--- nova/db/sqlalchemy/migrate_repo/versions/162_cern_network.py  1970-01-01 01:00:00.000000000 +0100
+++ nova/db/sqlalchemy/migrate_repo/versions/162_cern_network.py  2014-06-09 11:26:08.000000000 +0200
@@ -0,0 +1,55 @@
+# This is a placeholder for Grizzly backports.
+# Do not use this number for new Havana work.  New Havana work starts after
+# all the placeholders.
+#
+# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
+# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html
+
+
+
+from sqlalchemy import Boolean, Column, DateTime, Integer
+from sqlalchemy import MetaData, String, Table
+from nova.openstack.common import log as logging
+
+meta = MetaData()
+
+cern_network = Table('cern_network', meta,
+        Column('created_at', DateTime(timezone=False)),
+        Column('updated_at', DateTime(timezone=False)),
+        Column('deleted_at', DateTime(timezone=False)),
+        Column('deleted', Boolean(create_constraint=True, name=None)),
+        Column('id', Integer, primary_key=True),
+        Column('netcluster', String(255)),
+        Column('host', String(255)),
+        )
+
+# (fixed_ips)
+column_mac = Column('mac',  String(255))
+column_netcluster = Column('netcluster',  String(255))
+
+
+def upgrade(migrate_engine):
+    meta.bind = migrate_engine
+
+    # create CERN tables
+    for table in (cern_network, ):
+        try:
+            table.create()
+        except Exception:
+            pass
+
+    # alter fixed_ips table
+    table = Table('fixed_ips', meta, autoload=True)
+    try:
+        table.create_column(column_mac)
+    except Exception:
+        pass
+
+    try:
+        table.create_column(column_netcluster)
+    except Exception:
+        pass
+
+def downgrade(migration_engine):
+    pass
+
--- nova/db/sqlalchemy/migrate_repo/versions/162_placeholder.py   2014-04-03 20:49:46.000000000 +0200
+++ nova/db/sqlalchemy/migrate_repo/versions/162_placeholder.py   1970-01-01 01:00:00.000000000 +0100
@@ -1,14 +0,0 @@
-# This is a placeholder for Grizzly backports.
-# Do not use this number for new Havana work.  New Havana work starts after
-# all the placeholders.
-#
-# See https://blueprints.launchpad.net/nova/+spec/backportable-db-migrations
-# http://lists.openstack.org/pipermail/openstack-dev/2013-March/006827.html
-
-
-def upgrade(migrate_engine):
-    pass
-
-
-def downgrade(migration_engine):
-    pass
--- nova/db/sqlalchemy/models.py  2014-04-03 20:49:46.000000000 +0200
+++ nova/db/sqlalchemy/models.py  2014-06-09 11:27:15.000000000 +0200
@@ -865,7 +865,19 @@
                                 'FixedIp.instance_uuid == Instance.uuid,'
                                 'FixedIp.deleted == 0,'
                                 'Instance.deleted == 0)')
-
+
+    mac = Column(String(255), unique=True)
+    netcluster = Column(String(255))
+
+
+
+class CernNetwork(BASE, NovaBase):
+    """Represents an Ip Service configuration at CERN."""
+    __tablename__ = 'cern_network'
+    id = Column(Integer, primary_key=True)
+    netcluster = Column(String(255))
+    host = Column(String(255), unique=True)
+
 
 class FloatingIp(BASE, NovaBase):
     """Represents a floating ip that dynamically forwards to a fixed ip."""
--- nova/exception.py 2014-04-03 20:49:46.000000000 +0200
+++ nova/exception.py 2014-06-09 11:36:38.000000000 +0200
@@ -1475,3 +1475,45 @@
 
 class KeyManagerError(NovaException):
     msg_fmt = _("key manager error: %(reason)s")
+
+
+class CernProjectTargetCell(NovaException):
+    msg_fmt = _("Failed to select available cell.")
+
+
+class CernDNS(NovaException):
+    msg_fmt = _("Failed to update DNS.")
+
+
+class CernNetwork(NovaException):
+    msg_fmt = _("Network inconsistency.")
+
+
+class CernHostnameWrong(NovaException):
+    msg_fmt = _("Invalid hostname.")
+
+
+class CernInvalidHostname(Invalid):
+    msg_fmt = _("Invalid hostname.")
+    code = 404
+
+
+class CernInvalidUser(Invalid):
+    msg_fmt = _("Invalid user.")
+    code = 404
+
+
+class CernInvalidEgroup(Invalid):
+    msg_fmt = _("Invalid egroup.")
+    code = 404
+
+
+class CernInvalidUserEgroup(Invalid):
+    msg_fmt = _("Invalid user or egroup.")
+    code = 404
+
+
+class CernLanDB(NovaException):
+    msg_fmt = _("Unable to connect to LanDB")
+
+
--- nova/network/linux_net.py 2014-04-03 20:49:46.000000000 +0200
+++ nova/network/linux_net.py 2014-06-09 11:38:03.000000000 +0200
@@ -396,7 +396,18 @@
         # the snat chain.
         self.ipv4['nat'].add_chain('float-snat')
         self.ipv4['nat'].add_rule('snat', '-j $float-snat')
-
+
+        self.ipv4['filter'].add_rule('INPUT',
+                                     '-s %s -j ACCEPT' %(CONF.metadata_host))
+        self.ipv4['filter'].add_rule('FORWARD',
+                                     '-m pkttype --pkt-type broadcast -j ACCEPT')
+        self.ipv4['nat'].add_rule('PREROUTING',
+                                  '-s 0.0.0.0/0 -d 169.254.169.254/32 '
+                                  '-p tcp -m tcp --dport 80 -j DNAT '
+                                  '--to-destination %s:%s' %
+                                          (CONF.metadata_host,
+                                           CONF.metadata_port))
+
     def defer_apply_on(self):
         self.iptables_apply_deferred = True
 
--- nova/network/manager.py   2014-04-03 20:49:46.000000000 +0200
+++ nova/network/manager.py   2014-06-09 18:11:43.000000000 +0200
@@ -78,6 +78,13 @@
 from nova import quota
 from nova import servicegroup
 from nova import utils
+
+from nova import cern
+import nova.image.glance
+import time
+import string
+import random
+
 
 LOG = logging.getLogger(__name__)
 
@@ -162,9 +169,18 @@
                default='nova.network.l3.LinuxNetL3',
                help="Indicates underlying L3 management library"),
     ]
-
+
+cern_opts = [
+    cfg.BoolOpt('cern_landb',
+               default=True,
+               help='Update instances information in landb'),
+    ]
+
 CONF = cfg.CONF
 CONF.register_opts(network_opts)
+
+CONF.register_opts(cern_opts)
+
 CONF.import_opt('use_ipv6', 'nova.netconf')
 CONF.import_opt('my_ip', 'nova.netconf')
 CONF.import_opt('network_topic', 'nova.network.rpcapi')
@@ -1933,3 +1949,390 @@
         """Number of reserved ips at the top of the range."""
         parent_reserved = super(VlanManager, self)._top_reserved_ips
         return parent_reserved + CONF.cnt_vpn_clients
+
+
+class CernManager(NetworkManager):
+    """CERN Network Manager."""
+
+    timeout_fixed_ips = False
+
+    required_create_args = ['bridge']
+
+    def _setup_network_on_host(self, context, network):
+        """Setup Network on this host."""
+        net = {}
+        net['injected'] = CONF.flat_injected
+        self.db.network_update(context, network['id'], net)
+
+    def _teardown_network_on_host(self, context, network):
+        """Tear down network on this host."""
+        pass
+
+    def get_floating_ip(self, context, id):
+        """Returns a floating IP as a dict."""
+        # NOTE(vish): This is no longer used but can't be removed until
+        #             we major version the network_rpcapi to 2.0.
+        return None
+
+    def get_floating_pools(self, context):
+        """Returns list of floating pools."""
+        # NOTE(maurosr) This method should be removed in future, replaced by
+        # get_floating_ip_pools. See bug #1091668
+        return {}
+
+    def get_floating_ip_pools(self, context):
+        """Returns list of floating ip pools."""
+        # NOTE(vish): This is no longer used but can't be removed until
+        #             we major version the network_rpcapi to 2.0.
+        return {}
+
+    def get_floating_ip_by_address(self, context, address):
+        """Returns a floating IP as a dict."""
+        # NOTE(vish): This is no longer used but can't be removed until
+        #             we major version the network_rpcapi to 2.0.
+        return None
+
+    def get_floating_ips_by_project(self, context):
+        """Returns the floating IPs allocated to a project."""
+        # NOTE(vish): This is no longer used but can't be removed until
+        #             we major version the network_rpcapi to 2.0.
+        return []
+
+    def get_floating_ips_by_fixed_address(self, context, fixed_address):
+        """Returns the floating IPs associated with a fixed_address."""
+        # NOTE(vish): This is no longer used but can't be removed until
+        #             we major version the network_rpcapi to 2.0.
+        return []
+
+    @network_api.wrap_check_policy
+    def allocate_floating_ip(self, context, project_id, pool):
+        """Gets a floating ip from the pool."""
+        return None
+
+    @network_api.wrap_check_policy
+    def deallocate_floating_ip(self, context, address,
+                               affect_auto_assigned):
+        """Returns a floating ip to the pool."""
+        return None
+
+    @network_api.wrap_check_policy
+    def associate_floating_ip(self, context, floating_address, fixed_address,
+                              affect_auto_assigned=False):
+        """Associates a floating ip with a fixed ip.
+
+        Makes sure everything makes sense then calls _associate_floating_ip,
+        rpc'ing to correct host if i'm not it.
+        """
+        return None
+
+    @network_api.wrap_check_policy
+    def disassociate_floating_ip(self, context, address,
+                                 affect_auto_assigned=False):
+        """Disassociates a floating ip from its fixed ip.
+
+        Makes sure everything makes sense then calls _disassociate_floating_ip,
+        rpc'ing to correct host if i'm not it.
+        """
+        return None
+
+    def migrate_instance_start(self, context, instance_uuid,
+                               floating_addresses,
+                               rxtx_factor=None, project_id=None,
+                               source=None, dest=None):
+        pass
+
+    def migrate_instance_finish(self, context, instance_uuid,
+                                floating_addresses, host=None,
+                                rxtx_factor=None, project_id=None,
+                                source=None, dest=None):
+        pass
+
+    def update_dns(self, context, network_ids):
+        """Called when fixed IP is allocated or deallocated."""
+        pass
+
+    def create_networks(self, context, label, cidr, multi_host, num_networks,
+                        network_size, cidr_v6, gateway, gateway_v6, bridge,
+                        bridge_interface, dns1=None, dns2=None, **kwargs):
+        """Create network."""
+
+        net = {}
+        net['bridge'] = bridge
+        net['bridge_interface'] = bridge_interface
+        net['multi_host'] = multi_host
+        net['dns1'] = dns1
+        net['dns2'] = dns2
+        net['label'] = label
+        net['cidr'] = cidr
+        net['gateway'] = gateway
+
+        network = self.db.network_create_safe(context, net)
+
+    def update_network(self, context, cluster_name):
+
+        client_landb = cern.LanDB()
+        vms = client_landb.vmClusterGetDevices(cluster_name)
+        services = client_landb.vmClusterGetInfo(cluster_name).Services
+
+        primary_service = ""
+        for srv in services:
+            try:
+                if primary_service == "":
+                    primary_service = (client_landb.getServiceInfo(srv)).\
+                                        Primary.__str__()
+                    continue
+                if primary_service != (client_landb.getServiceInfo(srv)).\
+                                        Primary.__str__():
+                    LOG.error(_("Network cluster has different primary "
+                                 "services"))
+                    raise exception.CernNetwork()
+            except Exception as e:
+                LOG.error(_("Cannot get network primary service for network "
+                            "cluster - %s - %s"), cluster_name, str(e))
+                raise exception.CernNetwork()
+
+        physical_devices = client_landb.getDevicesFromService(primary_service)
+
+        hosts = []
+        for host in physical_devices:
+            if self.db.cern_netcluster_get(context, host+'.CERN.CH') == None:
+                LOG.info(_("Adding host - %s" % str(host)))
+                hosts.append({'host':host+'.CERN.CH', 'netcluster':cluster_name})
+            else:
+                LOG.debug(_("Device already exists in DB - %s" % str(host)))
+
+        fixed_ips = []
+        for vm in vms:
+            try:
+                vm_info = client_landb.getDeviceInfo(vm)
+            except Exception as e:
+                LOG.error(_("Cannot get VM netwok info - %s" % str(e)))
+                raise exception.CernNetwork()
+
+            ip = vm_info.Interfaces[0].IPAddress.__str__()
+            mac = (vm_info.NetworkInterfaceCards[0].HardwareAddress.__str__()).replace('-', ':')
+
+            if self.db.cern_fixed_ip_get_by_address(context, ip) == None:
+                LOG.info(_("Adding ip - %s" % str(ip)))
+
+                fixed_ips.append({'network_id': '1',
+                                  'address': ip,
+                                  'reserved': False,
+                                  'mac': mac,
+                                  'netcluster': cluster_name})
+
+            else:
+                LOG.debug(_("IP already exists in DB - %s" % str(ip)))
+
+        self.db.cern_fixed_host_bulk_create(context, hosts)
+        self.db.fixed_ip_bulk_create(context, fixed_ips)
+
+    def allocate_for_instance(self, context, **kwargs):
+
+        instance_uuid = kwargs['instance_id']
+        host = kwargs['host']
+        project_id = kwargs['project_id']
+        rxtx_factor = kwargs['rxtx_factor']
+        requested_networks = kwargs.get('requested_networks')
+        vpn = kwargs['vpn']
+        admin_context = context.elevated()
+        LOG.debug(_("network allocations"), instance_uuid=instance_uuid,
+                  context=context)
+        networks = self._get_networks_for_instance(admin_context,
+                                        instance_uuid, project_id,
+                                        requested_networks=requested_networks)
+        LOG.debug(_('networks retrieved for instance: |%(networks)s|'),
+                  locals(), context=context, instance_uuid=instance_uuid)
+
+        vm_hostname = (self.db.instance_get_by_uuid(context, instance_uuid))['hostname']
+        vm_display = (self.db.instance_get_by_uuid(context, instance_uuid))['display_name']
+        if vm_hostname.lower() != vm_display.lower():
+            LOG.error(_("Hostname is not valid: %s - %s" % (vm_hostname.lower(), vm_display.lower())))
+            raise exception.CernHostnameWrong()
+
+        self._allocate_address(admin_context, instance_uuid, host, networks,
+                               vm_hostname.lower())
+
+        return self.get_instance_nw_info(context, instance_uuid, rxtx_factor,
+                                         host)
+
+    def deallocate_fixed_ip(self, context, address, host, teardown=True):
+        """Deallocate ip and vif"""
+
+        self._deallocate_address(context, address, host, teardown)
+
+    def _allocate_address(self, admin_context, instance_uuid, host, networks,
+                          vm_name):
+        """Allocates instance address."""
+
+        ipservice =  self.db.cern_netcluster_get(admin_context, host)
+
+        for i in range(20):
+            cern_address = self.db.cern_mac_ip_get(admin_context,
+                                                 ipservice['netcluster'], host)
+            vm_ip = cern_address['address']
+            vm_mac = cern_address['mac']
+            network = networks[0]
+
+            LOG.info(_("Selected IP |%s| and MAC |%s| for instance |%s| "
+                       "on host |%s|" % (vm_ip, vm_mac, instance_uuid, host)))
+
+            vif = {'address': vm_mac,
+                   'instance_uuid': instance_uuid,
+                   'network_id': network['id'],
+                   'uuid': str(str(uuid.uuid4()))}
+
+            try:
+                self.db.virtual_interface_create(admin_context, vif)
+            except Exception as e:
+                LOG.info(_("IP |%s| and MAC |%s| for instance |%s| was already "
+                "reserved. Selecting other." % (vm_ip, vm_mac, instance_uuid)))
+                continue
+            break
+        else:
+            raise exception.VirtualInterfaceMacAddressException()
+
+        get_vif = self.db.virtual_interface_get_by_instance_and_network
+        vifx = get_vif(admin_context, instance_uuid, network['id'])
+        values = {'allocated': True,
+                  'virtual_interface_id': vifx['id'],
+                  'instance_uuid': instance_uuid}
+        self.db.fixed_ip_update(admin_context, vm_ip, values)
+
+        if not CONF.cern_landb:
+            return
+
+        client_landb = cern.LanDB()
+        client_xldap = cern.Xldap()
+
+        if client_landb.device_exists(vm_name):
+            LOG.error(_("Hostname already exists in landb: %s" % vm_name))
+            raise exception.CernHostnameWrong()
+
+        metadata = self.db.instance_metadata_get(admin_context, instance_uuid)
+        device_name = client_landb.device_hostname(vm_ip)
+
+        client_landb.device_migrate(device_name,
+                             (host.lower()).replace('.cern.ch', ''))
+
+        if ('cern-update-hostname' in metadata.keys()\
+                and metadata['cern-update-hostname'].lower() == 'false'):
+            landb_hostname = client_landb.device_hostname(vm_ip)
+            update_dict = {}
+            update_dict['display_name'] = landb_hostname.strip()
+            update_dict['hostname'] = landb_hostname.strip()
+            instance = instance_obj.Instance.get_by_uuid(admin_context,
+                                                     instance_uuid)
+            instance.update(update_dict)
+            instance.save()
+            vm_name = device_name
+
+        try:
+            image_id = (self.db.instance_get_by_uuid(admin_context,
+                                                instance_uuid))['image_ref']
+        except Exception as e:
+            LOG.info(_("Cannot get image id for instance %s - %s" % (str(instance_uuid), str(e))))
+
+        if image_id:
+            image_service = nova.image.glance.get_default_image_service()
+            image_metadata = image_service.show(admin_context, image_id)
+        else:
+            image_metadata = {}
+
+        landb_operating_system = {'Name':'LINUX', 'Version': 'UNKNOWN'}
+        if 'properties' in image_metadata.keys()\
+                         and 'os' in image_metadata['properties'].keys():
+            if (image_metadata['properties']['os']).lower() == 'windows':
+                landb_operating_system = {'Name':'WINDOWS', 'Version': 'UNKNOWN'}
+                if 'os_version' in image_metadata['properties'].keys() \
+        and 'server' in (image_metadata['properties']['os_version']).lower():
+                    landb_operating_system = {'Name':'WINDOWS', 'Version': 'SERVER'}
+
+        user_name = self.db.instance_get_by_uuid(admin_context,
+                                                 instance_uuid)["user_id"]
+        person_id = client_xldap.user_exists(user_name)
+        if not person_id:
+            LOG.error(_("Cannot verify if USER exists: %s" % user_name))
+            raise exception.CernInvalidUser()
+        landb_responsible = {'PersonID':person_id}
+        landb_mainuser = {'PersonID':person_id}
+
+        if 'landb-mainuser' in metadata.keys():
+            landb_update = True
+
+            user_id = client_xldap.user_exists(metadata['landb-mainuser'])
+            egroup_id = client_xldap.egroup_exists(metadata['landb-mainuser'])
+
+            if user_id:
+                landb_mainuser = {'PersonID':user_id}
+            elif egroup_id:
+                landb_mainuser = {'FirstName':'E-GROUP', 'Name':egroup_id}
+            else:
+                LOG.error(_("Cannot find user/egroup for main user"))
+                raise exception.CernInvalidUserEgroup()
+
+        if 'landb-responsible' in metadata.keys():
+            landb_update = True
+
+            user_id = client_xldap.user_exists(metadata['landb-responsible'])
+            egroup_id = client_xldap.egroup_exists(metadata['landb-responsible'])
+
+            if user_id:
+                landb_responsible = {'PersonID':user_id}
+            elif egroup_id:
+                landb_responsible = {'FirstName':'E-GROUP', 'Name':egroup_id}
+            else:
+                LOG.error(_("Cannot find user/egroup for responsible user"))
+                raise exception.CernInvalidUserEgroup()
+
+        landb_description = ""
+        if 'landb-description' in metadata.keys():
+            landb_update = True
+            landb_description = metadata['landb-description']
+
+        client_landb.vm_update(device_name,
+                               new_device=vm_name,
+                               description=landb_description,
+                               operating_system=landb_operating_system,
+                               responsible_person=landb_responsible,
+                               user_person=landb_mainuser)
+
+        if 'landb-alias' in metadata.keys():
+            new_alias = [x.strip() for x in metadata['landb-alias'].split(',')]
+            client_landb.alias_update(vm_name, new_alias)
+        else:
+            client_landb.alias_update(vm_name, [])
+
+
+    def _deallocate_address(self, context, address, host, teardown):
+
+        fixed_ip_ref = self.db.fixed_ip_get_by_address(context, address)
+        vif_id = fixed_ip_ref['virtual_interface_id']
+
+        instance = self.db.instance_get_by_uuid(
+                context.elevated(read_deleted='yes'),
+                fixed_ip_ref['instance_uuid'])
+
+        self._do_trigger_security_group_members_refresh_for_instance(
+            instance['uuid'])
+
+        if self._validate_instance_zone_for_dns_domain(context, instance):
+            for n in self.instance_dns_manager.get_entries_by_address(address,
+                                                     self.instance_dns_domain):
+                self.instance_dns_manager.delete_entry(n,
+                                                      self.instance_dns_domain)
+
+        self.db.fixed_ip_update(context, address,
+                                {'allocated': False,
+                                 'virtual_interface_id': None,
+                                 'host': None})
+
+        network = self._get_network_by_id(context, fixed_ip_ref['network_id'])
+        self._teardown_network_on_host(context, network)
+
+        self.db.fixed_ip_disassociate(context, address)
+
+        client_landb = cern.LanDB()
+        device_name = client_landb.device_hostname(address)
+        client_landb.vm_delete(device_name)
+
--- nova/scheduler/filters/image_props_filter.py  2014-04-03 20:49:46.000000000 +0200
+++ nova/scheduler/filters/image_props_filter.py  2014-06-09 11:43:24.000000000 +0200
@@ -38,7 +38,9 @@
 
     def _instance_supported(self, host_state, image_props):
         img_arch = image_props.get('architecture', None)
-        img_h_type = image_props.get('hypervisor_type', None)
+# CERN
+        img_h_type = image_props.get('hypervisor_type', 'qemu')
+# CERN
         img_vm_mode = image_props.get('vm_mode', None)
         checked_img_props = (img_arch, img_h_type, img_vm_mode)
 
--- nova/scheduler/filters/project_to_aggregate_filter.py 1970-01-01 01:00:00.000000000 +0100
+++ nova/scheduler/filters/project_to_aggregate_filter.py 2014-06-09 11:45:09.000000000 +0200
@@ -0,0 +1,57 @@
+# Copyright (c) 2011-2013 OpenStack Foundation
+# All Rights Reserved.
+#
+#    Licensed under the Apache License, Version 2.0 (the "License"); you may
+#    not use this file except in compliance with the License. You may obtain
+#    a copy of the License at
+#
+#         http://www.apache.org/licenses/LICENSE-2.0
+#
+#    Unless required by applicable law or agreed to in writing, software
+#    distributed under the License is distributed on an "AS IS" BASIS, WITHOUT
+#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the
+#    License for the specific language governing permissions and limitations
+#    under the License.
+
+from nova import db
+from nova.openstack.common.gettextutils import _
+from nova.openstack.common import log as logging
+from nova.scheduler import filters
+
+LOG = logging.getLogger(__name__)
+
+
+class ProjectsToAggregateFilter(filters.BaseHostFilter):
+    """Isolate projects in aggregates."""
+
+    def host_passes(self, host_state, filter_properties):
+        """If the metadata key "projects_to_aggregate" is defined in an
+        aggregate only instances from the specified projects can be created on
+        the aggregate hosts.
+        """
+        spec = filter_properties.get('request_spec', {})
+        props = spec.get('instance_properties', {})
+        project_id = props.get('project_id')
+        context = filter_properties['context'].elevated()
+        aggr_meta = db.aggregate_metadata_get_by_host(context,
+                                 host_state.host, key='projects_to_aggregate')
+        if aggr_meta != {}:
+            aggr_meta_ids = aggr_meta['projects_to_aggregate'].pop()
+            filter_projects_id = [x.strip() for x in aggr_meta_ids.split(',')]
+            if project_id not in filter_projects_id:
+                LOG.debug(_("%(host_state)s fails project id on "
+                    "aggregate"), {'host_state': host_state})
+                return False
+        else:
+            aggrs_meta = db.aggregate_metadata_get_all_by_key(context,
+                                                      'projects_to_aggregate')
+            aggrs_projects_id = []
+            for projects_id in aggrs_meta['projects_to_aggregate']:
+                for x in projects_id.split(','):
+                    aggrs_projects_id.append(x.strip())
+            if project_id in aggrs_projects_id:
+                LOG.debug(_("%(host_state)s fails project id on "
+                    "aggregate"), {'host_state': host_state})
+                return False
+        return True
+
--- nova/virt/firewall.py 2014-04-03 20:49:46.000000000 +0200
+++ nova/virt/firewall.py 2014-06-09 11:49:12.000000000 +0200
@@ -170,8 +170,12 @@
     def unfilter_instance(self, instance, network_info):
         if self.instances.pop(instance['id'], None):
             # NOTE(vish): use the passed info instead of the stored info
+
             self.network_infos.pop(instance['id'])
+            self.network_infos[instance['id']] = network_info
             self.remove_filters_for_instance(instance)
+            self.network_infos.pop(instance['id'])
+
             self.iptables.apply()
         else:
             LOG.info(_('Attempted to unfilter instance which is not '
@@ -249,14 +253,32 @@
                                                             network_info)
         self._add_filters('local', ipv4_rules, ipv6_rules)
         self._add_filters(chain_name, inst_ipv4_rules, inst_ipv6_rules)
-
+
+        try:
+            v4_subnets = self._get_subnets(network_info, 4)
+            ips_v4 = [ip['address'] for subnet in v4_subnets
+                                for ip in subnet['ips']]
+            self.iptables.ipv4['filter'].add_rule('FORWARD', '-s %s -d 0.0.0.0/0 -j ACCEPT' % (ips_v4[0],))
+        except Exception as e:
+            LOG.warn(_("Failed to add firewall rule. %s" % str(e)))
+            pass
+
     def remove_filters_for_instance(self, instance):
         chain_name = self._instance_chain_name(instance)
 
         self.iptables.ipv4['filter'].remove_chain(chain_name)
         if CONF.use_ipv6:
             self.iptables.ipv6['filter'].remove_chain(chain_name)
-
+
+        network_info = self.network_infos[instance['id']]
+        try:
+            v4_subnets = self._get_subnets(network_info, 4)
+            ips_v4 = [ip['address'] for subnet in v4_subnets
+                                for ip in subnet['ips']]
+            self.iptables.ipv4['filter'].remove_rule('FORWARD', '-s %s -d 0.0.0.0/0 -j ACCEPT' % (ips_v4[0],))
+        except Exception as e:
+            LOG.warn(_("Cannot remove firewall rule. %s" % str(e)))
+
     @staticmethod
     def _security_group_chain_name(security_group_id):
         return 'nova-sg-%s' % (security_group_id,)
--- nova/virt/libvirt/firewall.py 2014-04-03 20:49:46.000000000 +0200
+++ nova/virt/libvirt/firewall.py 2014-06-09 11:51:56.000000000 +0200
@@ -219,6 +219,10 @@
         filter_set = ['no-mac-spoofing',
                       'no-ip-spoofing',
                       'no-arp-spoofing']
+
+        self._define_filter(self.nova_no_nd_reflection_filter)
+        filter_set.append('nova-no-nd-reflection')
+
         if CONF.use_ipv6:
             self._define_filter(self.nova_no_nd_reflection_filter)
             filter_set.append('nova-no-nd-reflection')
@@ -319,8 +323,11 @@
         # Overriding base class method for applying nwfilter operation
         if self.instances.pop(instance['id'], None):
             # NOTE(vish): use the passed info instead of the stored info
-            self.network_infos.pop(instance['id'])
+
+            self.network_infos[instance['id']] = network_info
             self.remove_filters_for_instance(instance)
+            self.network_infos.pop(instance['id'])
+
             self.iptables.apply()
             self.nwfilter.unfilter_instance(instance, network_info)
         else:
